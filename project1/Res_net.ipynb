{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb25d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from models.digit_classifier import DigitClassifier\n",
    "from utils import accuracy\n",
    "from utils import train_model_double_objective\n",
    "from importlib import reload\n",
    "from models.net3 import Net3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee9a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlc_practical_prologue import generate_pair_sets\n",
    "\n",
    "nSamples = 1000\n",
    "data = generate_pair_sets(nSamples)\n",
    "var_names = [\"train_input\", \"train_target\", \"train_classes\", \"test_input\", \"test_target\", \"test_classes\"]\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = data\n",
    "stringWidth = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7bc891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-train model for classifying digits\n",
    "from models.digit_classifier import DigitClassifier\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils import train_model\n",
    "\n",
    "mini_batch_size = 100\n",
    "model_classifier = DigitClassifier(out_h = 10, subnet = False)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_classifier.parameters(), lr = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_in = train_input[:, 0, :, :].unsqueeze(axis = 1)\n",
    "train_class = train_classes[:,0]\n",
    "test_in = test_input[:, 0, :, :].unsqueeze(axis = 1)\n",
    "test_class = test_classes[:,0]\n",
    "\n",
    "train_model(model = model_classifier, \n",
    "            train_input = train_in, train_target = train_class,\n",
    "            test_input = test_in, test_target = test_class,\n",
    "            nb_epochs = 20, mini_batch_size = 100, \n",
    "            optimizer = optimizer, criterion = criterion,\n",
    "            verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88185f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/relja/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 78.465 / train accuracy 62.0%, test accuracy 59.6\n",
      "Epoch 1: loss 45.323 / train accuracy 65.5%, test accuracy 65.6\n",
      "Epoch 2: loss 36.257 / train accuracy 67.0%, test accuracy 65.5\n",
      "Epoch 3: loss 28.062 / train accuracy 69.5%, test accuracy 68.1\n",
      "Epoch 4: loss 21.097 / train accuracy 76.7%, test accuracy 74.2\n",
      "Epoch 5: loss 17.770 / train accuracy 81.2%, test accuracy 78.6\n",
      "Epoch 6: loss 15.645 / train accuracy 83.4%, test accuracy 80.4\n",
      "Epoch 7: loss 12.584 / train accuracy 86.4%, test accuracy 81.0\n",
      "Epoch 8: loss 11.352 / train accuracy 87.6%, test accuracy 82.9\n",
      "Epoch 9: loss 9.866 / train accuracy 88.2%, test accuracy 84.3\n"
     ]
    }
   ],
   "source": [
    "model = Net3(nb_residual_blocks = 20, model_class_digit, c1 = 32, c2 = 32, c3 = 64, h = 100, p = 0.3, hidden_channel_1 = 1, hidden_channel_2 = 10)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "eta = 1e-3\n",
    "mini_batch_size = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = eta)\n",
    "nb_epochs = 10\n",
    "\n",
    "train_model_double_objective(model, train_input, train_target, \n",
    "                             train_classes, test_input, test_target, test_classes, \n",
    "                             nb_epochs, mini_batch_size, optimizer, criterion, criterion2, \n",
    "                             beta = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336af53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.20\n"
     ]
    }
   ],
   "source": [
    "from utils import accuracy_of_digit_class\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_of_digit_class(model, test_input, test_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "867f256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 29605 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "nParams = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('The model has {:d} trainable parameters'.format(nParams))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
